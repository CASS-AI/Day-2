# Machine-Learning-Interpretation
This repository is made to save Mohammad Amin Dadgar's Presentation files and sources. <br>
Machine learning interpretation is an act of interpreting results into more informative and understandable results. We presented why an AI machine produces certain outcomes and how to make it more understandable by humans to be able to trust them.

## Methods and challenges that we've introduced
- Have an introduction on first, second and third wave of Artificial Intelligence
- Represent the pros and cons of each wave
- Answered to the question that why second wave AI has become complicated
- Explained the Interpretation and Interpretability.
- Demonstrated a few examples for the LIME interpretation algorithm.
- And for the final words Described the future of Artificial Intelligence: Explainable AI (XIA)

## References used
[1]  https://www.darpa.mil/about-us/darpa-perspective-on-ai <br>
[2] Selvaraju, R.R., Cogswell, M., Das, A. et al. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. Int J Comput Vis 128, 336–359 (2020). https://doi.org/10.1007/s11263-019-01228-7 <br>
[3] https://github.com/amindadgar/Spaceship-titanic-prediction <br>
[4] Ribeiro, M., Singh, S. and Guestrin, C., 2022. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. [online] arXiv.org. https://doi.org/10.48550/arXiv.1602.04938. <br>
[5] Lime examples at https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html <br>
[6] Alonso Moral, J.M., Castiello, C., Magdalena, L., Mencar, C. (2021). Toward Explainable Artificial Intelligence Through Fuzzy Systems. In: Explainable Fuzzy Systems. Studies in Computational Intelligence, vol 970. Springer, Cham. https://doi.org/10.1007/978-3-030-71098-9_1



